{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22793cd1-09a3-4c1d-81fd-b0f5384b5ec4",
   "metadata": {},
   "source": [
    "<!-- =============================================== -->\n",
    "<!--                     DAY 27                      -->\n",
    "<!-- =============================================== -->\n",
    "\n",
    "## Day 27 — Linear Regression (Simple & Multiple) + OLS Recap\n",
    "\n",
    "This notebook continues my **Machine Learning Learning Journey** with a structured\n",
    "recap of **Linear Regression**, focusing on **model formulation, geometry, and\n",
    "Ordinary Least Squares (OLS)** from an implementation and optimization perspective.\n",
    "\n",
    "---\n",
    "\n",
    "## Topics Covered\n",
    "\n",
    "- Parametric vs Non-Parametric algorithms  \n",
    "- Linear Regression as a parametric model  \n",
    "- Assumption of linear model form  \n",
    "- Simple Linear Regression (single feature)  \n",
    "- Multiple Linear Regression (multiple features)  \n",
    "- Relationship between:\n",
    "  - features  \n",
    "  - weights (coefficients)  \n",
    "  - target variable  \n",
    "\n",
    "- Linear Regression equation:\n",
    "  - \\( y = w_1x_1 + w_2x_2 + \\dots + w_nx_n + w_0 \\)\n",
    "\n",
    "- Geometric interpretation:\n",
    "  - One feature → straight line  \n",
    "  - Two features → plane  \n",
    "  - Multiple features → hyperplane  \n",
    "\n",
    "- Interpretation of coefficients:\n",
    "  - Feature importance  \n",
    "  - Effect of scaling  \n",
    "  - Additive contribution of features  \n",
    "\n",
    "- Role of intercept (bias term):\n",
    "  - Baseline adjustment  \n",
    "  - Shifting predictions  \n",
    "  - Bias reduction  \n",
    "\n",
    "- Vector and matrix representation of Linear Regression  \n",
    "- Dot product form:\n",
    "  - \\( y = w^T x + w_0 \\)\n",
    "\n",
    "- Homogeneous coordinates:\n",
    "  - Converting intercept into weight  \n",
    "  - Adding constant feature (1) to dataset  \n",
    "\n",
    "- Feature matrix and weight vector formulation  \n",
    "- How Linear Regression fits a hyperplane  \n",
    "\n",
    "---\n",
    "\n",
    "## Ordinary Least Squares (OLS)\n",
    "\n",
    "- Objective of OLS:\n",
    "  - Minimize squared error  \n",
    "- Loss function:\n",
    "  - Squared Error  \n",
    "  - Mean Squared Error (MSE)  \n",
    "\n",
    "- Why squared error is used:\n",
    "  - Penalizes large errors  \n",
    "  - Avoids sign cancellation  \n",
    "\n",
    "- OLS optimization objective:\n",
    "  - \\( \\min (y - \\hat{y})^2 \\)\n",
    "\n",
    "- Estimating coefficients:\n",
    "  - Weights \\( w_1, w_2, \\dots, w_n \\)  \n",
    "  - Intercept \\( w_0 \\)\n",
    "\n",
    "---\n",
    "\n",
    "## Solving OLS\n",
    "\n",
    "- Closed-form (matrix) solution  \n",
    "- Normal equation concept  \n",
    "- Limitations of matrix inversion  \n",
    "- Over-determined systems  \n",
    "- Non-square matrices  \n",
    "\n",
    "- Moore–Penrose pseudo-inverse  \n",
    "- Role of Singular Value Decomposition (SVD)  \n",
    "- Numerical stability considerations  \n",
    "\n",
    "---\n",
    "\n",
    "## Optimization Perspective\n",
    "\n",
    "- When closed-form solution is expensive  \n",
    "- Gradient Descent as an alternative  \n",
    "- Iterative optimization approach  \n",
    "- Scalability to large datasets  \n",
    "\n",
    "- Library implementations:\n",
    "  - `LinearRegression`  \n",
    "  - `SGDRegressor`\n",
    "\n",
    "---\n",
    "\n",
    "## Simple vs Multiple Linear Regression\n",
    "\n",
    "- Simple Linear Regression:\n",
    "  - One feature + intercept  \n",
    "\n",
    "- Multiple Linear Regression:\n",
    "  - Multiple features  \n",
    "  - Hyperplane fitting  \n",
    "\n",
    "---\n",
    "\n",
    "## Notebook\n",
    "- `Day_27_Linear_Regression_OLS_Recap.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "*Day 27 reinforces Linear Regression fundamentals and OLS optimization,\n",
    "bridging mathematical intuition with practical machine learning workflows.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764afd71-de80-4080-a2e9-7209d3988ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
