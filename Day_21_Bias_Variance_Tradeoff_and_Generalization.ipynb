{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10760a7f-852c-4e10-8945-12d1da57ce35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T18:05:23.286918Z",
     "iopub.status.busy": "2026-01-03T18:05:23.285916Z",
     "iopub.status.idle": "2026-01-03T18:05:23.309202Z",
     "shell.execute_reply": "2026-01-03T18:05:23.308173Z",
     "shell.execute_reply.started": "2026-01-03T18:05:23.286918Z"
    }
   },
   "source": [
    "# Bias–Variance Tradeoff & Generalization (Extension)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Regression Evaluation Metrics\n",
    "- Loss Functions\n",
    "  - MAE (Mean Absolute Error)\n",
    "  - MSE (Mean Squared Error)\n",
    "  - RMSE (Root Mean Squared Error)\n",
    "  - MAPE\n",
    "- Purpose of loss functions in regression\n",
    "\n",
    "---\n",
    "\n",
    "## 2. R² Score (Coefficient of Determination)\n",
    "- Definition:\n",
    "  - Explained Variance / Total Variance\n",
    "- Interpretation:\n",
    "  - Value ranges from (-∞ to 1)\n",
    "  - R² ≈ 0 → model explains very little variance\n",
    "  - R² ≈ 1 → model explains most variance\n",
    "- Meaning of explained vs unexplained variance\n",
    "- Limitations of R²:\n",
    "  - Always increases when new features are added\n",
    "  - Does not indicate feature usefulness\n",
    "- Why R² alone is not sufficient\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Adjusted R²\n",
    "- Why Adjusted R² is needed\n",
    "- Penalizes unnecessary features\n",
    "- Increases only when a new feature adds real explanatory power\n",
    "- Formula intuition:\n",
    "  - Depends on:\n",
    "    - Number of observations (n)\n",
    "    - Number of features (k)\n",
    "- Preferred over R² for multiple regression models\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Model Building Pipeline\n",
    "- Data → ML Algorithm → Model → Predictions (ŷ)\n",
    "- Comparing predictions (ŷ) with actual values (y)\n",
    "- Performance evaluation using metrics\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Training vs Testing Performance\n",
    "- Training performance:\n",
    "  - Measures how well model fits training data\n",
    "- Testing performance:\n",
    "  - Measures generalization ability\n",
    "- Large gap between train and test performance → memorization\n",
    "\n",
    "---\n",
    "\n",
    "## 6. What is Generalization?\n",
    "- Ability of a model to perform well on unseen data\n",
    "- Goal of Machine Learning:\n",
    "  - Not just high accuracy\n",
    "  - But good generalization\n",
    "- Difference between:\n",
    "  - Memorization\n",
    "  - Learning general patterns\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Overfitting and Underfitting\n",
    "- Overfitting:\n",
    "  - Very high training performance\n",
    "  - Poor testing performance\n",
    "- Underfitting:\n",
    "  - Poor performance on both train and test data\n",
    "- Relation to model complexity\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Role of Cross-Validation\n",
    "- Used to assess generalization\n",
    "- Helps detect overfitting\n",
    "- Provides more reliable performance estimate\n",
    "- Train/Test split vs Cross-validation\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Population vs Sample Concept\n",
    "- Population:\n",
    "  - Entire real-world data (unknown)\n",
    "- Sample:\n",
    "  - Subset drawn from population\n",
    "- Training data as a representative sample\n",
    "- Assumption:\n",
    "  - Training data ≈ population data\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Representative Sample\n",
    "- Importance of:\n",
    "  - Random sampling\n",
    "  - Independence\n",
    "- Sample statistics approximating population parameters\n",
    "- Bias introduced due to non-representative samples\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Population Model vs Sample Model\n",
    "- Population model:\n",
    "  - Ideal generalized model\n",
    "  - Unknown in practice\n",
    "- Sample model:\n",
    "  - Trained on available data\n",
    "- Goal:\n",
    "  - Sample model ≈ Population model\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Model Comparison Using Training Data\n",
    "- Building multiple models on training data\n",
    "- Selecting model closest to population behavior\n",
    "- Risk of choosing overly complex or overly simple models\n",
    "\n",
    "---\n",
    "\n",
    "## 13. Bias\n",
    "- Definition:\n",
    "  - Error due to wrong assumptions\n",
    "- Difference between predicted value and true value\n",
    "- Characteristics:\n",
    "  - High bias → underfitting\n",
    "  - Simple models\n",
    "- Examples:\n",
    "  - Mean model\n",
    "  - Very shallow Decision Trees\n",
    "  - kNN with very large k\n",
    "\n",
    "---\n",
    "\n",
    "## 14. Variance\n",
    "- Definition:\n",
    "  - Sensitivity of model to data changes\n",
    "- Characteristics:\n",
    "  - High variance → overfitting\n",
    "  - Complex models\n",
    "- Small changes in data → large changes in predictions\n",
    "- Examples:\n",
    "  - kNN with k = 1\n",
    "  - Deep Decision Trees\n",
    "\n",
    "---\n",
    "\n",
    "## 15. Effect of Hyperparameters\n",
    "- k in k-Nearest Neighbors\n",
    "- max_depth in Decision Trees\n",
    "- Hyperparameters control:\n",
    "  - Model complexity\n",
    "  - Bias–variance balance\n",
    "\n",
    "---\n",
    "\n",
    "## 16. Bias–Variance Tradeoff\n",
    "- Simple models:\n",
    "  - High bias, low variance\n",
    "- Complex models:\n",
    "  - Low bias, high variance\n",
    "- Goal:\n",
    "  - Find optimal balance\n",
    "- Cannot minimize both bias and variance simultaneously\n",
    "\n",
    "---\n",
    "\n",
    "## 17. Model Stability\n",
    "- Stable models:\n",
    "  - Predictions don’t change much with data changes\n",
    "- Unstable models:\n",
    "  - Highly sensitive to training data\n",
    "- Relation to variance\n",
    "\n",
    "---\n",
    "\n",
    "## 18. Train vs Test Error Behavior\n",
    "- High bias model:\n",
    "  - Train error high\n",
    "  - Test error high\n",
    "- High variance model:\n",
    "  - Train error low\n",
    "  - Test error high\n",
    "- Well-generalized model:\n",
    "  - Train error ≈ Test error (both low)\n",
    "\n",
    "---\n",
    "\n",
    "## 19. Summary of Bias vs Variance\n",
    "| Aspect | Bias | Variance |\n",
    "|------|------|----------|\n",
    "| Model Type | Simple | Complex |\n",
    "| Error Source | Wrong assumptions | Sensitivity to data |\n",
    "| Train Error | High | Low |\n",
    "| Test Error | High | High |\n",
    "| Risk | Underfitting | Overfitting |\n",
    "\n",
    "---\n",
    "\n",
    "## 20. Final Takeaway\n",
    "- Machine Learning is about:\n",
    "  - Learning general patterns\n",
    "  - Not memorizing data\n",
    "- Bias–Variance tradeoff is central to:\n",
    "  - Model selection\n",
    "  - Hyperparameter tuning\n",
    "  - Achieving good generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d088d8-0467-47aa-a3f5-ae22bee3a6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
