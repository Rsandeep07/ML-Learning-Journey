{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22793cd1-09a3-4c1d-81fd-b0f5384b5ec4",
   "metadata": {},
   "source": [
    "# Decision Tree — CART Algorithm (Topics Covered)\n",
    "\n",
    "This notebook is part of my **Machine Learning Learning Journey** and introduces\n",
    "the **Decision Tree algorithm using CART**, focusing on intuition, workflow,\n",
    "and geometric understanding.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Recap: K-Nearest Neighbors (KNN)\n",
    "\n",
    "- KNN as a **distance-based algorithm**\n",
    "- Effect of `k` (number of neighbors) on:\n",
    "  - Decision boundary\n",
    "  - Model performance\n",
    "- Role of `weights`:\n",
    "  - Uniform\n",
    "  - Distance-based\n",
    "- Relationship between:\n",
    "  - k value\n",
    "  - weights\n",
    "  - bias–variance tradeoff\n",
    "- KNN as a **non-parametric, lazy learner**\n",
    "- No explicit training phase in KNN\n",
    "- Hyperparameters in KNN:\n",
    "  - `k`\n",
    "  - `weights`\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Parameters vs Hyperparameters\n",
    "\n",
    "- **Parameters**\n",
    "  - Learned during training\n",
    "  - Example: model rules or splits\n",
    "- **Hyperparameters**\n",
    "  - Set before training\n",
    "  - Used to control model capacity and performance\n",
    "- Hyperparameters help improve:\n",
    "  - Bias–variance balance\n",
    "  - Generalization ability\n",
    "\n",
    "---\n",
    "\n",
    "## 3. CART Algorithm Overview\n",
    "\n",
    "- CART = **Classification and Regression Trees**\n",
    "- Used for:\n",
    "  - Classification\n",
    "  - Regression\n",
    "- Informally known as:\n",
    "  - **Decision Tree**\n",
    "- Decision Trees are:\n",
    "  - Supervised learning algorithms\n",
    "  - Rule-based models\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Decision Tree vs KNN\n",
    "\n",
    "- KNN:\n",
    "  - Similarity-based\n",
    "  - Uses distance metrics\n",
    "  - Lazy learning\n",
    "- Decision Tree:\n",
    "  - Rule-based\n",
    "  - Uses feature-based questions\n",
    "  - Explicit model building\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Decision Tree as a Model\n",
    "\n",
    "- Decision Tree is a **non-parametric method**\n",
    "- Can model **complex, non-linear relationships**\n",
    "- Does not assume any functional form\n",
    "- Learns decision rules directly from data\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Supervised Learning Context\n",
    "\n",
    "- Input: Feature set\n",
    "- Output: Predicted label or value\n",
    "- Applicable for:\n",
    "  - Classification\n",
    "  - Regression\n",
    "- Learns mapping:\n",
    "  - Features → Target\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Decision Tree Structure\n",
    "\n",
    "- Root Node\n",
    "- Internal (Decision) Nodes\n",
    "- Leaf Nodes\n",
    "- Branches\n",
    "- Flow of prediction:\n",
    "  - Root → Decisions → Leaf\n",
    "\n",
    "---\n",
    "\n",
    "## 8. If–Then–Else Logic\n",
    "\n",
    "- Decision Trees work using **if–then–else rules**\n",
    "- Each node represents a question\n",
    "- Each branch represents an answer\n",
    "- Final decision made at leaf node\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Why Decision Trees?\n",
    "\n",
    "- Highly interpretable models\n",
    "- Easy to explain to non-technical stakeholders\n",
    "- Widely used in:\n",
    "  - Healthcare\n",
    "  - Finance\n",
    "  - Risk assessment\n",
    "- Allows:\n",
    "  - Feature importance interpretation\n",
    "  - Decision backtracking\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Human Decision-Making Analogy\n",
    "\n",
    "- Decision Trees mimic **human thinking**\n",
    "- Humans make decisions by:\n",
    "  - Asking a series of questions\n",
    "- Decision Trees formalize this process\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Example: Rainy Season Decision Logic\n",
    "\n",
    "- Real-world example of decision-making\n",
    "- Sequential questions such as:\n",
    "  - Is it raining?\n",
    "  - Did it rain recently?\n",
    "  - Is there a predicted rainfall?\n",
    "- Demonstrates:\n",
    "  - Binary branching\n",
    "  - Logical flow\n",
    "  - If–else decision making\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Dataset Representation\n",
    "\n",
    "- Dataset structure:\n",
    "  - Features: \\(x_1, x_2, \\dots, x_n\\)\n",
    "  - Target: \\(y\\)\n",
    "- Decision Tree built by:\n",
    "  - Asking questions on feature values\n",
    "  - Splitting data based on feature thresholds\n",
    "\n",
    "---\n",
    "\n",
    "## 13. Feature-Based Questions\n",
    "\n",
    "- Questions asked on:\n",
    "  - Feature values\n",
    "  - Feature ranges\n",
    "- Example:\n",
    "  - Is \\(x_1 > 2.5\\)?\n",
    "- Each question leads to a split\n",
    "\n",
    "---\n",
    "\n",
    "## 14. Binary Splits in CART\n",
    "\n",
    "- CART always performs **binary splits**\n",
    "- Each node has:\n",
    "  - True branch\n",
    "  - False branch\n",
    "- Even for categorical features\n",
    "\n",
    "---\n",
    "\n",
    "## 15. CART vs Other Tree Algorithms\n",
    "\n",
    "- CART:\n",
    "  - Binary splits only\n",
    "- ID3:\n",
    "  - Uses information gain\n",
    "- CHAID:\n",
    "  - Allows multi-way splits\n",
    "- CART is simpler and more consistent\n",
    "\n",
    "---\n",
    "\n",
    "## 16. Nodes and Branches\n",
    "\n",
    "- Root Node:\n",
    "  - First decision\n",
    "- Internal Nodes:\n",
    "  - Intermediate decisions\n",
    "- Leaf Nodes:\n",
    "  - Final prediction\n",
    "- Branches:\n",
    "  - Outcome of a decision\n",
    "\n",
    "---\n",
    "\n",
    "## 17. Worked Example (Height, Weight, Gender)\n",
    "\n",
    "- Dataset with:\n",
    "  - Height (cm)\n",
    "  - Weight (kg)\n",
    "  - Gender (target)\n",
    "- Demonstrates:\n",
    "  - Root node selection\n",
    "  - Threshold-based splits\n",
    "  - Leaf node predictions\n",
    "\n",
    "---\n",
    "\n",
    "## 18. Backtracking in Decision Trees\n",
    "\n",
    "- Ability to trace:\n",
    "  - Why a prediction was made\n",
    "- Identify:\n",
    "  - Which features influenced the decision\n",
    "- Useful for:\n",
    "  - Explainability\n",
    "  - Debugging models\n",
    "\n",
    "---\n",
    "\n",
    "## 19. Decision Tree Prediction Mechanism\n",
    "\n",
    "- Predictions are made by:\n",
    "  - Traversing the tree\n",
    "  - Following decision rules\n",
    "- No distance computation involved\n",
    "\n",
    "---\n",
    "\n",
    "## 20. Decision Boundary Representation\n",
    "\n",
    "- Decision Trees partition feature space\n",
    "- Splits are:\n",
    "  - Axis-parallel\n",
    "- Creates rectangular decision regions\n",
    "- Different from curved boundaries in KNN\n",
    "\n",
    "---\n",
    "\n",
    "## 21. Training Phase of Decision Trees\n",
    "\n",
    "- Training involves:\n",
    "  - Recursive partitioning of data\n",
    "  - Asking questions at each node\n",
    "- Dataset is divided into:\n",
    "  - Smaller and purer subsets\n",
    "\n",
    "---\n",
    "\n",
    "## 22. Feature Space Partitioning\n",
    "\n",
    "- Feature space divided using:\n",
    "  - Axis-parallel lines\n",
    "- Each split corresponds to a rule\n",
    "- Regions map to leaf nodes\n",
    "\n",
    "---\n",
    "\n",
    "## 23. Data Preprocessing Requirements\n",
    "\n",
    "- No feature scaling required\n",
    "- No distance-based calculations\n",
    "- Only requirement:\n",
    "  - Categorical encoding (if needed)\n",
    "\n",
    "---\n",
    "\n",
    "## 24. Key Observations\n",
    "\n",
    "- Decision Trees are interpretable and intuitive\n",
    "- No need for feature scaling\n",
    "- Can handle non-linear relationships\n",
    "- Sensitive to data and splits\n",
    "- Foundation for ensemble methods:\n",
    "  - Random Forest\n",
    "  - Gradient Boosting\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764afd71-de80-4080-a2e9-7209d3988ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
