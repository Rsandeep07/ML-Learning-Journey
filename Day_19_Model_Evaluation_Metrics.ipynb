{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2134b5e-9d79-4275-8d90-6b55fe8d3d0f",
   "metadata": {},
   "source": [
    "## Model Evaluation Metrics – Classification & Regression (Deep Dive)\n",
    "\n",
    "- This notebook focuses on a **detailed and practical understanding of evaluation metrics** used in **classification and regression problems**, with emphasis on **metric selection based on problem context**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bdd2b2-c535-47a3-befc-abd822d324b4",
   "metadata": {},
   "source": [
    "## Learning Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bd3375-f15e-4de6-9429-b98b9d7cb0d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T12:52:36.668466Z",
     "iopub.status.busy": "2025-12-31T12:52:36.667458Z",
     "iopub.status.idle": "2025-12-31T12:52:36.679640Z",
     "shell.execute_reply": "2025-12-31T12:52:36.677631Z",
     "shell.execute_reply.started": "2025-12-31T12:52:36.668466Z"
    }
   },
   "source": [
    "## Learning Objective\n",
    "\n",
    "The goal of this notebook is to:\n",
    "- Understand how different evaluation metrics work\n",
    "- Learn **when** and **why** to choose a specific metric\n",
    "- Analyze model performance beyond accuracy\n",
    "- Connect evaluation metrics with real-world decision making"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7657b305-2f0d-4282-ae52-192f1f47076c",
   "metadata": {},
   "source": [
    "## Classification Metrics Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0c4451-4ab3-47a1-a0e1-0711367dfc33",
   "metadata": {},
   "source": [
    "## Classification Metrics – Recap\n",
    "\n",
    "Before diving deeper, we briefly revisit core classification metrics:\n",
    "\n",
    "- **Precision** – Correctly predicted positive samples out of all predicted positives\n",
    "- **Recall** – Correctly predicted positive samples out of all actual positives\n",
    "- **F1 Score** – Harmonic mean of Precision and Recall\n",
    "\n",
    "These metrics are especially important when class distribution is **imbalanced**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1182ce82-15e9-4c7c-b3e2-0598ef71751c",
   "metadata": {},
   "source": [
    "## Cost of Misclassification (Key Concept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a698f8a2-84b9-485d-9066-bdf03c560812",
   "metadata": {},
   "source": [
    "## Cost of Misclassification (CoM)\n",
    "\n",
    "Choosing an evaluation metric depends on the **cost of incorrect predictions**:\n",
    "\n",
    "- **False Positive (FP)**: Predicting positive when it is actually negative\n",
    "- **False Negative (FN)**: Predicting negative when it is actually positive\n",
    "\n",
    "### Metric selection based on cost:\n",
    "- FP cost high → Focus on **Precision**\n",
    "- FN cost high → Focus on **Recall**\n",
    "- Both FP & FN important → Use **F1 Score**\n",
    "\n",
    "Metric choice should always align with the **objective of the ML problem**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c300139-fb44-41d6-9d5f-0bc075a47aaf",
   "metadata": {},
   "source": [
    "## Accuracy: When It Works & When It Fails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8057653c-9fce-49ad-9512-857479510ca5",
   "metadata": {},
   "source": [
    "## Accuracy – When Is It Suitable?\n",
    "\n",
    "Accuracy is a good metric **only when data is perfectly balanced**.\n",
    "\n",
    "### Balanced Dataset:\n",
    "- All classes have nearly equal proportions\n",
    "- Accuracy reasonably represents model performance\n",
    "\n",
    "### Imbalanced Dataset:\n",
    "- Accuracy can be misleading\n",
    "- Model may perform well on majority class but fail on minority class\n",
    "- Accuracy alone is **not sufficient**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e23c8a-da81-45e1-94c5-99591cc5aa9a",
   "metadata": {},
   "source": [
    "## Balanced Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88060905-695a-4a94-8a67-6ffacbc967fc",
   "metadata": {},
   "source": [
    "## Balanced Accuracy\n",
    "\n",
    "Balanced Accuracy is a modified accuracy metric used for **imbalanced datasets**.\n",
    "\n",
    "It is defined as the average of:\n",
    "- True Positive Rate (Recall)\n",
    "- True Negative Rate (Specificity)\n",
    "\n",
    "Balanced accuracy gives equal importance to all classes and provides a more reliable performance measure for imbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72d5bf8-2891-4523-be44-b9ca2aa0537e",
   "metadata": {},
   "source": [
    "## F1 Score and Fβ Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa1b07e-81f4-431c-89f6-e8eccb407d56",
   "metadata": {},
   "source": [
    "## F1 Score and Fβ Score\n",
    "\n",
    "### F1 Score\n",
    "- Harmonic mean of Precision and Recall\n",
    "- Suitable when both FP and FN are equally important\n",
    "\n",
    "### Fβ Score\n",
    "- Generalized version of F1 Score\n",
    "- β controls the importance of Recall vs Precision\n",
    "\n",
    "Interpretation:\n",
    "- β < 1 → Precision is more important\n",
    "- β = 1 → Precision and Recall equally important\n",
    "- β > 1 → Recall is more important\n",
    "\n",
    "The value of β depends on the **problem objective**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6248b56f-4ed0-44ae-b696-3d69db7998c4",
   "metadata": {},
   "source": [
    "## Baseline Model Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743644de-7754-4cad-8a97-dc348e6c19b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T12:57:22.426036Z",
     "iopub.status.busy": "2025-12-31T12:57:22.424637Z",
     "iopub.status.idle": "2025-12-31T12:57:22.438019Z",
     "shell.execute_reply": "2025-12-31T12:57:22.436417Z",
     "shell.execute_reply.started": "2025-12-31T12:57:22.426036Z"
    }
   },
   "source": [
    "## Baseline Model Concept\n",
    "A baseline model provides a **minimum benchmark** for model performance.\n",
    "\n",
    "Examples:\n",
    "- Classification → Predicting the majority class (mode)\n",
    "- Regression → Predicting the mean value\n",
    "\n",
    "Any trained model should **outperform the baseline model**.\n",
    "If a model performs worse than baseline, it is considered **not useful**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0ee432-e1c9-4dbd-8cf5-e2992929f11a",
   "metadata": {},
   "source": [
    "## Regression Metrics Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb65508-7233-42e4-9094-1663c961f2ed",
   "metadata": {},
   "source": [
    "## Regression Metrics\n",
    "\n",
    "In regression problems, the target variable is **numeric**.\n",
    "Model performance is evaluated using **error-based metrics**.\n",
    "\n",
    "Instead of comparing against the mean, we compare:\n",
    "- Predicted values (ŷ)\n",
    "- Actual values (y)\n",
    "\n",
    "All regression metrics are derived from **prediction errors**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084c6342-8b82-4b0c-96ba-b68503858f58",
   "metadata": {},
   "source": [
    "## Common Regression Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1466fc56-6dd5-4547-835d-366ba6e2176f",
   "metadata": {},
   "source": [
    "## Common Regression Metrics\n",
    "\n",
    "- **MAE (Mean Absolute Error)**  \n",
    "  Average absolute difference between predicted and actual values\n",
    "\n",
    "- **MSE (Mean Squared Error)**  \n",
    "  Average of squared prediction errors\n",
    "\n",
    "- **RMSE (Root Mean Squared Error)**  \n",
    "  Square root of MSE, penalizes larger errors more\n",
    "\n",
    "These metrics help quantify how far predictions are from actual values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93a72b7-f410-4c25-94ab-a283c2fa9299",
   "metadata": {},
   "source": [
    "## R² and Explained Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c7fdd3-f587-4b72-b57e-fdd1f3b14a7b",
   "metadata": {},
   "source": [
    "## R² (R-Squared)\n",
    "\n",
    "R² measures how well the regression model explains the variance in the data.\n",
    "\n",
    "- Compares model performance against a baseline mean model\n",
    "- Indicates the proportion of variance explained by the model\n",
    "\n",
    "### Error Decomposition:\n",
    "- Total Sum of Squares (TSS)\n",
    "- Explained Sum of Squares (ESS)\n",
    "- Residual Sum of Squares (RSS)\n",
    "\n",
    "R² = ESS / TSS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed24ada9-d6b4-44ab-8008-5b4ca2572b0f",
   "metadata": {},
   "source": [
    "## Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98af0505-0c7a-4468-a97f-4cfe877f7525",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- Metric selection depends on **problem context**, not convenience\n",
    "- Accuracy is unreliable for imbalanced datasets\n",
    "- Precision, Recall, and F-scores provide better insight\n",
    "- Models must always outperform a baseline\n",
    "- Regression metrics focus on prediction errors and explained variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7973092f-4bca-4063-9836-383d868e11a6",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e529e0-083c-400d-b4aa-e1c24986d0e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T13:00:14.399700Z",
     "iopub.status.busy": "2025-12-31T13:00:14.398688Z",
     "iopub.status.idle": "2025-12-31T13:00:14.412366Z",
     "shell.execute_reply": "2025-12-31T13:00:14.410343Z",
     "shell.execute_reply.started": "2025-12-31T13:00:14.399700Z"
    }
   },
   "source": [
    "## Conclusion\n",
    "This notebook builds a **strong evaluation mindset**, emphasizing:\n",
    "- Business-driven metric selection\n",
    "- Proper evaluation for imbalanced data\n",
    "- Understanding both classification and regression metrics\n",
    "\n",
    "Revisiting metrics at a deeper level strengthens overall ML model reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e41bd7-3e48-4d16-b390-6f25a629b768",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
