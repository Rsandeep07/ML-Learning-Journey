{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a52c0bfb-b0f5-4b24-a203-7295469e10f2",
   "metadata": {},
   "source": [
    "## Day 35 — Ensemble Learning: Boosting, AdaBoost & Gradient Boosting\n",
    "\n",
    "This notebook is part of my **Machine Learning Learning Journey** and focuses on\n",
    "**Boosting-based ensemble methods**.\n",
    "\n",
    "The session builds on Bagging and Random Forest concepts and explains:\n",
    "- Why boosting is needed\n",
    "- Sequential ensemble learning\n",
    "- Bias reduction using boosting\n",
    "- AdaBoost (Adaptive Boosting)\n",
    "- Gradient Boosting intuition\n",
    "- Overfitting control and regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff32d100-6d77-4986-a2dd-50bb179f9764",
   "metadata": {},
   "source": [
    "## 1. Recap: Ensemble Learning\n",
    "\n",
    "Ensemble Learning:\n",
    "> Combine multiple learners to build a strong model\n",
    "\n",
    "Key goals:\n",
    "- Improve generalization\n",
    "- Reduce bias or variance\n",
    "- Stabilize predictions\n",
    "\n",
    "Types:\n",
    "- Bagging → reduces variance\n",
    "- Boosting → reduces bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b423f4-8aeb-41de-9d95-1e73e49ff6bf",
   "metadata": {},
   "source": [
    "## 2. Why Boosting?\n",
    "\n",
    "Single models may suffer from:\n",
    "- High bias (underfitting)\n",
    "- Inability to learn complex patterns\n",
    "\n",
    "Boosting:\n",
    "- Learners are trained **sequentially**\n",
    "- Each new learner focuses on **previous errors**\n",
    "- Overall bias is reduced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3d3f62-39e9-4748-8c65-b30eef4bf953",
   "metadata": {},
   "source": [
    "## 3. Bagging vs Boosting\n",
    "\n",
    "| Aspect | Bagging | Boosting |\n",
    "|------|--------|----------|\n",
    "| Training | Parallel | Sequential |\n",
    "| Goal | Reduce variance | Reduce bias |\n",
    "| Data sampling | Bootstrap sampling | Reweighted samples |\n",
    "| Dependency | Independent learners | Dependent learners |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9165f1b-5e1d-4496-96b7-5fae429987a0",
   "metadata": {},
   "source": [
    "## 4. Sequential Learning in Boosting\n",
    "\n",
    "Workflow:\n",
    "1. Train weak learner 1\n",
    "2. Identify misclassified points\n",
    "3. Increase importance of misclassified samples\n",
    "4. Train next learner\n",
    "5. Repeat until error is minimized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdf0ab8-88f1-40a2-a53d-9f359d28daa3",
   "metadata": {},
   "source": [
    "## 5. Weak Learners\n",
    "\n",
    "Weak learner:\n",
    "- Slightly better than random guessing\n",
    "- High bias model\n",
    "\n",
    "Common choice:\n",
    "- Decision Tree with depth = 1\n",
    "- Also called **Decision Stump**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4ad416-b7e5-45b1-80f4-fd40a9e005f1",
   "metadata": {},
   "source": [
    "## 6. AdaBoost (Adaptive Boosting)\n",
    "\n",
    "AdaBoost is a **sequential ensemble algorithm** where:\n",
    "- Each learner focuses on previous mistakes\n",
    "- Misclassified samples get higher weight\n",
    "- Correctly classified samples get lower weight\n",
    "\n",
    "Base learner:\n",
    "- Decision Stump (1 split decision tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3256ebf8-d9aa-4c09-b45b-16bc72fe5e81",
   "metadata": {},
   "source": [
    "## 7. AdaBoost Training Intuition\n",
    "\n",
    "Steps:\n",
    "1. Assign equal weights to all samples\n",
    "2. Train weak learner\n",
    "3. Compute error rate\n",
    "4. Increase weight of misclassified points\n",
    "5. Train next learner on reweighted data\n",
    "6. Repeat until minimum error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac5f5ce-2c64-4eab-9a8c-3ad9538f7af6",
   "metadata": {},
   "source": [
    "## 8. AdaBoost Model Representation\n",
    "\n",
    "- Final model:\n",
    "\\[\n",
    "F(x) = \\alpha_1 h_1(x) + \\alpha_2 h_2(x) + \\dots + \\alpha_n h_n(x)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( h_i(x) \\) = weak learner\n",
    "- \\( \\alpha_i \\) = importance of learner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea24992-bb4a-4fb2-b9b1-ea6eaebf89a5",
   "metadata": {},
   "source": [
    "## 9. Boosting & Imbalanced Data\n",
    "\n",
    "Boosting naturally helps with imbalance:\n",
    "- Misclassified minority samples are reweighted\n",
    "- Minority patterns are learned more effectively\n",
    "- Reduces bias toward majority class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfed1c9-aa1c-4cad-a835-f85f7544515d",
   "metadata": {},
   "source": [
    "## 10. Gradient Boosting Motivation\n",
    "\n",
    "Instead of reweighting samples,\n",
    "Gradient Boosting:\n",
    "- Fits models on **residual errors**\n",
    "- Optimizes loss using **gradient descent**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe519eaf-ae9a-48a2-bbc7-602b6fecb526",
   "metadata": {},
   "source": [
    "## 11. Gradient Boosting Intuition\n",
    "\n",
    "Steps:\n",
    "1. Start with a simple model (mean prediction)\n",
    "2. Compute residuals (errors)\n",
    "3. Train shallow decision tree on residuals\n",
    "4. Add new tree to model\n",
    "5. Repeat sequentially\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2fa407-ae25-456a-82e9-ffad8125dd25",
   "metadata": {},
   "source": [
    "## 12. Learning Rate (η)\n",
    "\n",
    "Learning rate controls:\n",
    "- Contribution of each tree\n",
    "\n",
    "Small η:\n",
    "- Slower learning\n",
    "- Better generalization\n",
    "\n",
    "Large η:\n",
    "- Faster learning\n",
    "- Risk of overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc7e776-71ae-4fed-bf07-b838187ba59c",
   "metadata": {},
   "source": [
    "## 13. Overfitting in Gradient Boosting\n",
    "\n",
    "Risk:\n",
    "- Too many trees\n",
    "- Deep trees\n",
    "- High learning rate\n",
    "\n",
    "Solution:\n",
    "- Shallow trees\n",
    "- Low learning rate\n",
    "- Hyperparameter tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39052fdd-5c79-4050-a03c-f8f23885360c",
   "metadata": {},
   "source": [
    "## 14. Extreme Gradient Boosting (XGBoost)\n",
    "\n",
    "XGBoost improves Gradient Boosting by:\n",
    "- Parallelized tree construction\n",
    "- Regularization (L1 & L2)\n",
    "- Better handling of overfitting\n",
    "- GPU acceleration\n",
    "\n",
    "Used widely in competitions and industry.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb7a0c6-fda3-4cbb-ba85-18b5ec50943b",
   "metadata": {},
   "source": [
    "## AdaBoost Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9434d97-5f92-401b-9dd8-8e12c14f892d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T14:46:59.793970Z",
     "iopub.status.busy": "2026-02-03T14:46:59.793970Z",
     "iopub.status.idle": "2026-02-03T14:47:03.122154Z",
     "shell.execute_reply": "2026-02-03T14:47:03.122154Z",
     "shell.execute_reply.started": "2026-02-03T14:46:59.793970Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "ada = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=50,\n",
    "    learning_rate=1.0\n",
    ")\n",
    "\n",
    "ada.fit(X_train, y_train)\n",
    "print(\"AdaBoost Accuracy:\", accuracy_score(y_test, ada.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe18b6d-7804-497b-b7d4-32d111603b9f",
   "metadata": {},
   "source": [
    "## Gradient Boosting Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "484e7c69-21c8-47a5-853c-b12eeffe6aaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T14:47:22.957637Z",
     "iopub.status.busy": "2026-02-03T14:47:22.957637Z",
     "iopub.status.idle": "2026-02-03T14:47:23.552040Z",
     "shell.execute_reply": "2026-02-03T14:47:23.551683Z",
     "shell.execute_reply.started": "2026-02-03T14:47:22.957637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Accuracy: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3\n",
    ")\n",
    "\n",
    "gb.fit(X_train, y_train)\n",
    "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_test, gb.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef064716-d814-4301-a7d0-3c467684367b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Boosting is a sequential ensemble technique\n",
    "- Focuses on reducing bias\n",
    "- AdaBoost reweights misclassified samples\n",
    "- Gradient Boosting fits models on residuals\n",
    "- Learning rate controls contribution of trees\n",
    "- XGBoost adds regularization and parallelism\n",
    "- Boosting is powerful but prone to overfitting if not tuned\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
