{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ec8af50-7907-4621-a578-e22e4905be2b",
   "metadata": {},
   "source": [
    "# üìò Outlier Detection & Handling Imbalanced Data ‚Äî Notes\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Outlier Detection\n",
    "\n",
    "**Outliers** are data points that significantly deviate from the majority of data points.  \n",
    "They may arise due to:\n",
    "- Noise or measurement errors\n",
    "- Rare events\n",
    "- Data generation issues\n",
    "\n",
    "### üîπ Why detect outliers?\n",
    "- Outliers can influence the model heavily\n",
    "- They distort regression planes / decision boundaries\n",
    "- They may reduce model performance\n",
    "\n",
    "‚û°Ô∏è Hence, detect and handle outliers during preprocessing.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Popular Outlier Detection Algorithms\n",
    "\n",
    "Libraries:\n",
    "- **pyod** ‚Üí KNN and many advanced algorithms  \n",
    "- **sklearn** ‚Üí IsolationForest, LOF  \n",
    "\n",
    "---\n",
    "\n",
    "### 1Ô∏è‚É£ Isolation Forest\n",
    "\n",
    "üì¶ `sklearn.ensemble.IsolationForest`\n",
    "\n",
    "#### ‚úÖ Main Advantages\n",
    "- Very fast and scalable\n",
    "- No need of feature scaling\n",
    "- Works well for high dimensional data\n",
    "- Unsupervised method\n",
    "\n",
    "#### üîπ How it works\n",
    "- Builds a large number of random trees (Extra Trees)\n",
    "- Randomly selects feature and split value\n",
    "- Outliers are isolated at early levels (shorter path length)\n",
    "- Normal points require more splits\n",
    "\n",
    "#### üîπ Anomaly Score\n",
    "- Based on average path length in trees\n",
    "- Shorter path ‚Üí higher anomaly score ‚Üí outlier\n",
    "\n",
    "#### üîπ Contamination parameter\n",
    "- Defines expected fraction of outliers (e.g., 0.05)\n",
    "- Top anomaly scores based on contamination are marked as outliers\n",
    "- If not given, model estimates automatically (may vary like 10%, 20%)\n",
    "\n",
    "‚û°Ô∏è Usually we explicitly set contamination.\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ KNN-based Outlier Detection\n",
    "\n",
    "üì¶ Available in **pyod**\n",
    "\n",
    "#### üîπ Idea\n",
    "For each data point:\n",
    "1. Find k nearest neighbors (assume k = 5)\n",
    "2. Compute average distance:\n",
    "\n",
    "d_avg = (d_nn1 + d_nn2 + d_nn3 + d_nn4 + d_nn5) / 5\n",
    "\n",
    "- If d_avg is high ‚Üí point is far ‚Üí outlier\n",
    "\n",
    "#### ‚ùå Disadvantage\n",
    "- Computationally very expensive\n",
    "- Needs distance computation for all points\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ LOF ‚Äì Local Outlier Factor\n",
    "\n",
    "üì¶ `sklearn.neighbors.LocalOutlierFactor`\n",
    "\n",
    "#### üîπ Idea\n",
    "- Calculates local density for each observation\n",
    "- Compares density with its neighbors\n",
    "\n",
    "‚û°Ô∏è If density is much lower ‚Üí considered as outlier\n",
    "\n",
    "#### ‚úÖ Strength\n",
    "- Detects local outliers inside clusters\n",
    "- Also works for global outliers\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Summary: Outlier Methods\n",
    "\n",
    "| Algorithm | Key Idea | Pros | Cons |\n",
    "|-----------|----------|------|------|\n",
    "Isolation Forest | Early isolation using trees | Fast, scalable, no scaling | Needs contamination |\n",
    "KNN | Avg neighbor distance | Simple | Very slow |\n",
    "LOF | Local density | Finds local anomalies | Sensitive to k |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öñÔ∏è Imbalanced Data\n",
    "\n",
    "### üîπ What is Imbalanced Data?\n",
    "When categories of the target variable are not equally represented.\n",
    "\n",
    "Example:\n",
    "- Class A = 95% ‚Üí Majority\n",
    "- Class B = 5% ‚Üí Minority\n",
    "\n",
    "‚û°Ô∏è This dataset is highly imbalanced.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Why is it important?\n",
    "\n",
    "Most ML algorithms assume:\n",
    "> All classes are represented equally.\n",
    "\n",
    "But in imbalanced data:\n",
    "- Model becomes biased towards majority class\n",
    "- Learns majority patterns more frequently\n",
    "- Performs poorly on minority class\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Effects of Imbalanced Data\n",
    "- High accuracy but poor minority class recall\n",
    "- Model predicts majority class most of the time\n",
    "\n",
    "Example:\n",
    "If 95% = A, model predicts always A ‚Üí 95% accuracy but useless.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ When imbalance may not affect much (T&C)\n",
    "1. Dataset is sufficiently large  \n",
    "2. Data is linearly separable  \n",
    "\n",
    "‚ö†Ô∏è These cases are rare in practice.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç How to detect imbalance?\n",
    "\n",
    "Using pandas:\n",
    "```python\n",
    "y.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2a423f-e9ae-43bd-b410-00c191fe5873",
   "metadata": {},
   "source": [
    "### Imbalance Ratio (IR):\n",
    "IR = (# minority samples) / (# majority samples)\n",
    "\n",
    "Balanced ‚Üí IR = 1\n",
    "\n",
    "Highly imbalanced ‚Üí IR close to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e8415d-38ab-4693-9674-6409a895bb0a",
   "metadata": {},
   "source": [
    "### How to Handle Imbalanced Data?\n",
    "\n",
    "Three main approaches:\n",
    "\n",
    "- 1)Data-level methods\n",
    "- 2)Cost-sensitive learning\n",
    "- 3)Ensemble-based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984a1733-6c0e-4614-820a-1ccc6a015c08",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Data-Level Approach\n",
    "\n",
    "### üîπ Oversampling (Increase minority class)\n",
    "\n",
    "**Goal:** Make minority class size ‚âà majority class size  \n",
    "\n",
    "üì¶ **Library:** `imblearn`\n",
    "\n",
    "---\n",
    "\n",
    "### (a) Random Over Sampling (ROS)\n",
    "- Repeats minority class samples randomly  \n",
    "- Sends the same patterns again to the model  \n",
    "\n",
    "‚ùå **Disadvantages:**\n",
    "- Overfitting / memorization  \n",
    "- Increased training time  \n",
    "- No new information added  \n",
    "\n",
    "---\n",
    "\n",
    "### (b) SMOTE ‚Äì Synthetic Minority Oversampling Technique\n",
    "- Creates new synthetic samples  \n",
    "- Interpolates between minority class neighbors  \n",
    "\n",
    "**Variants:**\n",
    "- SMOTE-N  \n",
    "- SMOTE-NC (for categorical features)  \n",
    "- Borderline-SMOTE  \n",
    "- KNN-SMOTE  \n",
    "- SVM-SMOTE  \n",
    "\n",
    "‚úÖ Adds new information  \n",
    "‚ùå May create noisy samples near decision boundaries  \n",
    "\n",
    "---\n",
    "\n",
    "### (c) ADASYN ‚Äì Adaptive Synthetic Sampling\n",
    "- Generates more samples near difficult regions  \n",
    "- Focuses on hard-to-learn minority points  \n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Undersampling\n",
    "- Removes samples from the majority class  \n",
    "\n",
    "‚ùå **Risk:** Loss of useful information  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Important Rule\n",
    "‚úîÔ∏è Apply oversampling/undersampling **only on training data**  \n",
    "‚ùå Never apply on test data ‚Üí avoids data leakage  \n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Cost-Sensitive Learning\n",
    "- Assign higher cost to errors on minority class  \n",
    "- Many models support:\n",
    "\n",
    "```python\n",
    "class_weight='balanced'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1236fea7-f56b-44a4-b815-f6760dee3436",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ Ensemble-Based Methods\n",
    "\n",
    "- Combine multiple models designed for imbalanced data\n",
    "- Examples:\n",
    "    - Balanced Random Forest\n",
    "    - EasyEnsemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e51cb0f-4eaa-4c8e-aa8a-27a7fd0fcd30",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "- Isolation Forest ‚Üí fast, most used outlier detector\n",
    "- KNN ‚Üí intuitive but computationally expensive\n",
    "- LOF ‚Üí best for detecting local outliers\n",
    "- Imbalanced data biases models ‚Üí must be handled\n",
    "- Use SMOTE / ADASYN for oversampling\n",
    "- Use class weights for cost-sensitive learning\n",
    "- Use ensemble methods when needed\n",
    "- Always check class distribution using: #y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5386618e-e60b-41b3-bd7d-914be8bc8bd1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d5e5b2-d95a-4316-ba8f-7bccb61c2e99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
