{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bfa3a38-b170-45d9-9ab8-ffd1f54fe7b8",
   "metadata": {},
   "source": [
    "## Day 31 — Logistic Regression Case Study, Likelihood & Intro to SVM\n",
    "\n",
    "This notebook is part of my **Machine Learning Learning Journey** and focuses on a\n",
    "**Logistic Regression case study**, covering the **probabilistic foundation**,\n",
    "**likelihood formulation**, **loss derivation**, and **model assumptions**.\n",
    "\n",
    "The session also introduces the **intuition behind Support Vector Machines (SVM)**\n",
    "as a transition to margin-based classifiers.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258c625d-d052-4649-ac57-9d02c40b54d4",
   "metadata": {},
   "source": [
    "## 1. Recap: Logistic Regression Model\n",
    "\n",
    "Linear model:\n",
    "\\[\n",
    "z = w_1x_1 + w_2x_2 + \\dots + w_nx_n + w_0\n",
    "\\]\n",
    "\n",
    "Logistic Regression applies a transformation:\n",
    "\\[\n",
    "\\log\\left(\\frac{p}{1-p}\\right) = w^Tx + w_0\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\(p = P(y=1|x)\\)\n",
    "- Output is probability, not a raw value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc127d5c-080c-4adc-bf6f-2ab21a23cbc2",
   "metadata": {},
   "source": [
    "## 2. Why Logistic Regression?\n",
    "\n",
    "- Linear Regression gives unbounded output\n",
    "- Classification requires probabilities in [0, 1]\n",
    "- Logistic Regression provides:\n",
    "  - Credible feature importance\n",
    "  - Statistical interpretation\n",
    "  - Probabilistic decision making\n",
    "\n",
    "Logistic Regression is a **parametric classification algorithm**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf1afd0-b851-4bfb-ad1d-213055b457ce",
   "metadata": {},
   "source": [
    "## 3. Log-Odds (Logit) Transformation\n",
    "\n",
    "Odds:\n",
    "\\[\n",
    "\\text{odds} = \\frac{p}{1-p}\n",
    "\\]\n",
    "\n",
    "Log-odds:\n",
    "\\[\n",
    "\\log\\left(\\frac{p}{1-p}\\right) = w^Tx + w_0\n",
    "\\]\n",
    "\n",
    "Applying inverse transformation (Sigmoid):\n",
    "\\[\n",
    "p = \\frac{1}{1 + e^{-(w^Tx + w_0)}}\n",
    "\\]\n",
    "\n",
    "This maps any real value to (0, 1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e611ef54-b7c8-4c38-9ee8-732f184928f2",
   "metadata": {},
   "source": [
    "## 4. Classification Rule\n",
    "\n",
    "Default threshold:\n",
    "- \\(p \\ge 0.5 \\rightarrow y = 1\\)\n",
    "- \\(p < 0.5 \\rightarrow y = 0\\)\n",
    "\n",
    "Note:\n",
    "- Threshold is **not always 0.5**\n",
    "- Threshold selection depends on:\n",
    "  - ROC Curve\n",
    "  - Cross-validation\n",
    "  - Business cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c921de06-67a8-4463-8b3d-c71cb3de6dd0",
   "metadata": {},
   "source": [
    "## 5. Case Study: Student Marks Example\n",
    "\n",
    "Feature:\n",
    "- Hours studied\n",
    "\n",
    "Target:\n",
    "- Pass / Fail\n",
    "\n",
    "Linear model:\n",
    "\\[\n",
    "z = w_1 \\cdot \\text{hours} + w_0\n",
    "\\]\n",
    "\n",
    "Logistic model:\n",
    "\\[\n",
    "p = \\frac{1}{1 + e^{-z}}\n",
    "\\]\n",
    "\n",
    "Higher study hours → higher probability of passing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26c480d-b4f8-44f4-a91f-5fa53bee8d36",
   "metadata": {},
   "source": [
    "## 6. Why Squared Error is Not Used\n",
    "\n",
    "Squared Error:\n",
    "\\[\n",
    "(y - \\hat{y})^2\n",
    "\\]\n",
    "\n",
    "Problems:\n",
    "- Poor probabilistic interpretation\n",
    "- Non-convex for classification\n",
    "- Penalizes confident correct predictions\n",
    "\n",
    "Hence, **Likelihood-based optimization** is used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f521e5bb-5301-41f1-aa3d-b854c0842745",
   "metadata": {},
   "source": [
    "## 7. Probability of an Event\n",
    "\n",
    "For binary outcome:\n",
    "\\[\n",
    "y \\in \\{0, 1\\}\n",
    "\\]\n",
    "\n",
    "Probability of observing \\(y\\):\n",
    "\\[\n",
    "P(y|p) = p^y (1-p)^{(1-y)}\n",
    "\\]\n",
    "\n",
    "This formulation unifies both cases:\n",
    "- If \\(y=1\\) → probability = \\(p\\)\n",
    "- If \\(y=0\\) → probability = \\(1-p\\)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2f83db-d66e-4369-a9da-26cc8a14ca64",
   "metadata": {},
   "source": [
    "## 8. Likelihood Function\n",
    "\n",
    "For multiple independent observations:\n",
    "\\[\n",
    "L = \\prod_{i=1}^{m} p_i^{y_i}(1-p_i)^{(1-y_i)}\n",
    "\\]\n",
    "\n",
    "Goal:\n",
    "- Choose parameters that **maximize likelihood**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cdc0ff-faf5-4a59-a5d5-39fc52f1f89f",
   "metadata": {},
   "source": [
    "## 9. Log-Likelihood\n",
    "\n",
    "Applying log:\n",
    "\\[\n",
    "\\log L = \\sum_{i=1}^{m} \\left[\n",
    "y_i \\log(p_i) + (1-y_i)\\log(1-p_i)\n",
    "\\right]\n",
    "\\]\n",
    "\n",
    "Why log?\n",
    "- Converts product → sum\n",
    "- Numerically stable\n",
    "- Easier optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fbd05e-d0e9-4d93-97a4-77dac5b6b3ec",
   "metadata": {},
   "source": [
    "## 10. Negative Log Likelihood (NLL)\n",
    "\n",
    "Optimization objective:\n",
    "\\[\n",
    "\\min -\\sum_{i=1}^{m}\n",
    "\\left[\n",
    "y_i \\log(p_i) + (1-y_i)\\log(1-p_i)\n",
    "\\right]\n",
    "\\]\n",
    "\n",
    "This is called:\n",
    "- Binary Cross Entropy (BCE)\n",
    "- Log Loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662820c2-86da-404a-99ff-58343c9b3c41",
   "metadata": {},
   "source": [
    "## 11. Solving Logistic Regression\n",
    "\n",
    "Two approaches:\n",
    "- Maximum Likelihood Estimation (MLE)\n",
    "- Gradient Descent (practical)\n",
    "\n",
    "In practice:\n",
    "- `sklearn` uses numerical optimization\n",
    "- Deep Learning models use backpropagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bc1efb-4b94-4f58-8954-69c5f9a0fc3c",
   "metadata": {},
   "source": [
    "## 12. Assumptions of Logistic Regression\n",
    "\n",
    "1. Binary target variable  \n",
    "2. Independent observations  \n",
    "3. Linear relationship between features and log-odds  \n",
    "4. No multicollinearity  \n",
    "5. No influential outliers  \n",
    "6. Large sample size preferred  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921b8b60-60ac-4065-bb78-81fe162e6b67",
   "metadata": {},
   "source": [
    "## 13. Handling Multicollinearity & Feature Selection\n",
    "\n",
    "- Use VIF to detect multicollinearity\n",
    "- Feature selection:\n",
    "  - Manual (p-values < 0.05)\n",
    "  - Automatic (RFE)\n",
    "\n",
    "After fitting:\n",
    "- If train >> test performance → overfitting\n",
    "- Apply regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44a3fa0-ae2c-473b-af3f-8bf215018de6",
   "metadata": {},
   "source": [
    "## 14. Regularization\n",
    "\n",
    "- L1 (Lasso):\n",
    "  - Feature elimination\n",
    "- L2 (Ridge):\n",
    "  - Shrinks weights\n",
    "- Elastic Net:\n",
    "  - Combination of L1 & L2\n",
    "\n",
    "Used to:\n",
    "- Reduce overfitting\n",
    "- Improve generalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a17f3f7-034f-4b63-ad24-8bafca3cca70",
   "metadata": {},
   "source": [
    "## 15. Introduction to Support Vector Machines (SVM)\n",
    "\n",
    "Key idea:\n",
    "- Find the **best separating hyperplane**\n",
    "- Maximize margin between classes\n",
    "\n",
    "SVM focuses on:\n",
    "- Boundary points (support vectors)\n",
    "- Margin maximization\n",
    "- Robust classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a828272e-1d70-424f-aec7-63aafb3c30ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
